{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install imbalanced-learn\n",
        "!pip install delayed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r7UDKjpz-Fc",
        "outputId": "f7357aed-26a2-49d2-cac2-4138e7f38291"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "Collecting delayed\n",
            "  Downloading delayed-0.11.0b1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting hiredis (from delayed)\n",
            "  Downloading hiredis-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis (from delayed)\n",
            "  Downloading redis-5.0.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from redis->delayed) (4.0.3)\n",
            "Installing collected packages: redis, hiredis, delayed\n",
            "Successfully installed delayed-0.11.0b1 hiredis-2.2.3 redis-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import re\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Montar Google Drive para acceder a los archivos\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al archivo Excel en Google Drive\n",
        "ruta_archivo_excel = '/content/drive/MyDrive/Chatbots/songs_tagged.xlsx'\n",
        "\n",
        "# Cargar el archivo Excel en un DataFrame\n",
        "df = pd.read_excel(ruta_archivo_excel)\n",
        "\n",
        "\n",
        "#Preprocesar la columna 'seq', que contiene los lyrics de las canciones\n",
        "df[\"seq\"] = df[\"seq\"].astype(str) #Convertir la columna 'seq' a tipo de dato string\n",
        "df[\"seq\"] = df[\"seq\"].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x)) #Eliminar caracteres no alfabéticos de la columna\n",
        "\n",
        "#df = df.drop(df.index[-40000:], inplace=False)\n",
        "\n",
        "# Imprimir la estructura de los datos mostrando la primera fila\n",
        "print(df.head(1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYIkQQ7t0An2",
        "outputId": "7aa8718f-23bf-40cf-f124-bdb9a1c5cf8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Unnamed: 0        artist  \\\n",
            "0           0  Elijah Blake   \n",
            "\n",
            "                                                 seq      song  emotions  \n",
            "0  No noxD\\nI aint ever trapped out the bandoxD\\n...  Everyday  ['fear']  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular y mostrar la cuenta de valores únicos en la columna 'emotions'\n",
        "# normalize=True devuelve las proporciones en lugar de las frecuencias absolutas\n",
        "# dropna=False incluye los valores NaN en el conteo\n",
        "df['emotions'].value_counts(normalize=True, dropna=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z4I7kojgBCZ",
        "outputId": "85c2944e-ab1d-427f-dde0-973e09171e41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fear']        0.296447\n",
              "['sadness']     0.292680\n",
              "['neutral']     0.154999\n",
              "['anger']       0.109033\n",
              "['joy']         0.078082\n",
              "['surprise']    0.046237\n",
              "['disgust']     0.022522\n",
              "Name: emotions, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el DataFrame en características (X) y variable objetivo (y)\n",
        "X = df.drop('emotions', axis=1)# X contiene todas las columnas excepto 'emotions'\n",
        "y = df['emotions']# y contiene solo la columna 'emotions', que es la variable objetivo\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Se divide el conjunto de datos en conjuntos de entrenamiento y prueba usando train_test_split.\n",
        "# X_train y y_train son las características y la variable objetivo del conjunto de entrenamiento, respectivamente.\n",
        "# X_test e y_test son las características y la variable objetivo del conjunto de prueba, respectivamente.\n",
        "\n",
        "# Aplicar Random UnderSampler al conjunto de entrenamiento resampleado\n",
        "under_sampler = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = under_sampler.fit_resample(X_train, y_train)\n",
        "# Se aplica Random UnderSampler al conjunto de entrenamiento para equilibrar las clases.\n",
        "# Esto reduce aleatoriamente la cantidad de muestras de la clase mayoritaria para igualar la cantidad de la clase minoritaria.\n",
        "\n",
        "\n",
        "# Crear un nuevo DataFrame con los datos resampleados\n",
        "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='emotions')], axis=1)\n",
        "# Se crea un nuevo DataFrame (`df_resampled`) utilizando las características y la variable objetivo resampleadas."
      ],
      "metadata": {
        "id": "PlRSh_SYvI3s"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular y mostrar la cuenta de valores únicos en la columna 'emotions' del DataFrame resampleado\n",
        "# normalize=True devuelve las proporciones en lugar de las frecuencias absolutas\n",
        "# dropna=False incluye los valores NaN en el conteo\n",
        "df_resampled['emotions'].value_counts(normalize=True, dropna=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tzo_V6vvhW_",
        "outputId": "3f3ed3bc-59ed-40a7-836c-aeb390850cc2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger']       0.142857\n",
              "['disgust']     0.142857\n",
              "['fear']        0.142857\n",
              "['joy']         0.142857\n",
              "['neutral']     0.142857\n",
              "['sadness']     0.142857\n",
              "['surprise']    0.142857\n",
              "Name: emotions, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que df es tu DataFrame y tiene una columna 'emociones' que contiene listas de emociones\n",
        "# Primero, filtramos para mantener solo las filas con una emoción\n",
        "df_filtrado = df_resampled[df_resampled['emotions'].apply(lambda x: len(x) == 1)]\n",
        "\n",
        "# Convertimos la columna de listas en una columna de valores únicos\n",
        "df_filtrado['emotions'] = df_filtrado['emotions'].apply(lambda x: x[0])\n",
        "\n",
        "# Ahora, contamos las entradas para cada categoría de emoción\n",
        "conteo_emociones = df_filtrado['emotions'].value_counts()\n",
        "\n",
        "# Encontramos el número mínimo de entradas que una categoría de emoción tiene\n",
        "min_conteo = conteo_emociones.min()\n",
        "\n",
        "# Creamos un nuevo DataFrame vacío para las entradas balanceadas\n",
        "df_balanceado = pd.DataFrame()\n",
        "\n",
        "# Para cada emoción, tomamos una muestra del tamaño de la categoría más pequeña\n",
        "for emocion in conteo_emociones.index:\n",
        "    df_muestra = df_filtrado[df_filtrado['emotions'] == emocion].sample(min_conteo, replace=False)\n",
        "    df_balanceado = pd.concat([df_balanceado, df_muestra])\n",
        "\n",
        "# Restablecemos el índice del DataFrame resultante\n",
        "df_balanceado.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Ahora df_balanceado tiene un número igual de entradas para cada emoción\n"
      ],
      "metadata": {
        "id": "uqCGLl3AickG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the lyrics\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "encoded_data = tokenizer(df_resampled['seq'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=256)\n",
        "#Utiliza DistilBERT tokenizer para convertir las secuencias de texto 'seq' en representaciones numéricas\n",
        "\n",
        "# Prepare input tensors and labels\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_mask = encoded_data['attention_mask']\n",
        "labels = torch.tensor(df_resampled['emotions'].astype('category').cat.codes.tolist())  # Assuming 'emotions' is a categorical variable\n",
        "\n",
        "# Create a PyTorch dataset, con las representaciones numéricas y etiquetas de emociones\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders para los conjuntos de entrenamiento y validación, que se usarán en el entrenamiento\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, preentrenado para clasificaión de secuencias\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['emotions'].unique()))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop, itera sobre le conjunto de entrenamiento durante 6 épocas, realiza el entrenamiento y muestra la pérdida en una barra de progreso\n",
        "num_epochs = 6\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
        "\n",
        "    for batch in train_loader_tqdm:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loader_tqdm.set_postfix({'Loss': loss.item()})  # Display loss in the progress bar\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained('emotion_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_-wTJxJDHjF",
        "outputId": "74c82e1c-8485-4b62-b854-3b3fbab3f00d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|██████████| 118/118 [01:18<00:00,  1.50it/s, Loss=1.29]\n",
            "Epoch 2: 100%|██████████| 118/118 [01:19<00:00,  1.49it/s, Loss=1.12]\n",
            "Epoch 3: 100%|██████████| 118/118 [01:18<00:00,  1.50it/s, Loss=0.42]\n",
            "Epoch 4: 100%|██████████| 118/118 [01:18<00:00,  1.50it/s, Loss=0.234]\n",
            "Epoch 5: 100%|██████████| 118/118 [01:18<00:00,  1.50it/s, Loss=0.132]\n",
            "Epoch 6: 100%|██████████| 118/118 [01:18<00:00,  1.50it/s, Loss=0.0853]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model y tokenizer correspondiente\n",
        "model = DistilBertForSequenceClassification.from_pretrained('emotion_model')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define a list of prompts para las cuales se ha´ra la prediccion\n",
        "prompts = [\"I step on dog poop\", \"My husband is dead\", \"I graduated and feel so proud\"]\n",
        "\n",
        "# Iterate over each prompt\n",
        "for prompt in prompts:\n",
        "  # Tokenize the prompt\n",
        "  encoded_prompt = tokenizer(prompt, return_tensors='pt', max_length=256)\n",
        "\n",
        "  # Make a prediction using the trained model\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_prompt)\n",
        "\n",
        "  # Get the predicted emotion index\n",
        "  predicted_emotion_index = torch.argmax(model_output.logits).item()\n",
        "\n",
        "  # Map the index back to the emotion label using the DataFrame\n",
        "  predicted_emotion_label = df_resampled['emotions'].unique()[predicted_emotion_index]\n",
        "\n",
        "  # Get a song associated with the predicted emotion from the DaraFrame\n",
        "  result = df_resampled[df_resampled['emotions'] == predicted_emotion_label]\n",
        "\n",
        "  # Get the number of rows in the DataFrame\n",
        "  num_rows = result.shape[0]\n",
        "  #Generate a random index to select a random song from the DataFrame\n",
        "  random_index = np.random.randint(0, num_rows)\n",
        "\n",
        "  #Get the recommended song and artist\n",
        "  recommended_song = result['song'].iloc[random_index]\n",
        "  recommended_artist = result['artist'].iloc[random_index]\n",
        "\n",
        "  #Print the results\n",
        "  print(f\"Prompt: {prompt}\")\n",
        "  print(f\"Predicted Emotion: {predicted_emotion_label}\")\n",
        "  print(f\"Recommended Song: {recommended_song} - {recommended_artist}\")\n",
        "  print(\"--------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpyIoW_7DTHm",
        "outputId": "77b0e702-b4d2-4b6f-e7e6-a04cc43ecd69"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: I step on dog poop\n",
            "Predicted Emotion: ['disgust']\n",
            "Recommended Song: Tonight - Fabolous\n",
            "--------------------------------\n",
            "Prompt: My husband is dead\n",
            "Predicted Emotion: ['sadness']\n",
            "Recommended Song: Heart Wrenching Man - Thrush Hermit\n",
            "--------------------------------\n",
            "Prompt: I graduated and feel so proud\n",
            "Predicted Emotion: ['joy']\n",
            "Recommended Song: Flawless - The Neighbourhood\n",
            "--------------------------------\n"
          ]
        }
      ]
    }
  ]
}